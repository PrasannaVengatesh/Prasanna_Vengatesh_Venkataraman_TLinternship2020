{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install warcio\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from warcio.archiveiterator import ArchiveIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization of word\n",
    "WNL = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Stop words in english language\n",
    "eng_stop_words = stopwords.words('english')\n",
    "\n",
    "# Function to extract text from a URL and clean the same\n",
    "def data_extraction(url):\n",
    "    \n",
    "    # Extracting the text from a URL\n",
    "    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "    \n",
    "    # Removing tab space(s) and next line indicators\n",
    "    cleaned_text = re.sub(r'[\\n\\t]', '', soup.text).split(\" \")\n",
    "    \n",
    "    # Removing punctuations\n",
    "    cleaned_text = [re.sub(r'[^\\w\\s]', '', word) for word in cleaned_text if word != \"\"]\n",
    "    \n",
    "    # Remmvoing stop words\n",
    "    cleaned_text = [word for word in cleaned_text if word not in eng_stop_words]\n",
    "    \n",
    "    # Lemmatization of words\n",
    "    cleaned_text = \" \".join([WNL.lemmatize(word.lower()) for word in cleaned_text])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "# Function to identify cosine similarity between docs\n",
    "def get_cosine(vec1, vec2):\n",
    "    \n",
    "    # Numerator value calculation\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    \n",
    "    # Denominator value calculation\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URL's for articles relating to COVID 19 economic impact\n",
    "url_list = [\"https://www.census.gov/library/stories/2021/03/initial-impact-covid-19-on-united-states-economy-more-widespread-than-on-mortality.html\",\n",
    "            \"https://www.brookings.edu/research/explaining-the-economic-impact-of-covid-19-core-industries-and-the-hispanic-workforce/\",\n",
    "            \"https://www.brookings.edu/research/ten-facts-about-covid-19-and-the-u-s-economy/\",\n",
    "            \"https://www.bbc.com/news/business-51706225\",\n",
    "            \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7162753/\"]\n",
    "\n",
    "# String to concatenate and store all the text from above list of URLs after data cleaning\n",
    "final_doc = \"\"\n",
    "\n",
    "for url in url_list:\n",
    "    final_doc = final_doc + data_extraction(url)\n",
    "\n",
    "# Word Counter    \n",
    "WORD = re.compile(r\"\\w+\")\n",
    "t1 = Counter(WORD.findall(final_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13124\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:417: MarkupResemblesLocatorWarning: \"http://stream.quranicaudio.com/quran/abdullaah_alee_jaabir/072.mp3\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\13124\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:102: UserWarning: unknown status keyword 'M' in marked section\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(\"(youtu\\.be/|youtube\\.com/(watch\\?(.*\\&)?v=|(embed|v)/))([^?&\\\"'>]+)\")\n",
    "\n",
    "file_name = \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-10/segments/1581875141396.22/warc/CC-MAIN-20200216182139-20200216212139-00000.warc.gz\"\n",
    "\n",
    "stream = None\n",
    "try:\n",
    "    if file_name.startswith(\"http://\") or file_name.startswith(\"https://\"):\n",
    "        stream = requests.get(file_name, stream=True).raw\n",
    "    else:\n",
    "        stream = open(file_name, \"rb\")\n",
    "\n",
    "    final_url_output = []    \n",
    "\n",
    "    for record in ArchiveIterator(stream):\n",
    "    \n",
    "        if record.rec_type == \"warcinfo\":\n",
    "            continue\n",
    "\n",
    "        if not \".com/\" in record.rec_headers.get_header(\"WARC-Target-URI\"):\n",
    "            continue\n",
    "\n",
    "        contents = (record.content_stream().read().decode(\"utf-8\", \"replace\"))\n",
    "    \n",
    "        if contents != '':\n",
    "            soup = BeautifulSoup(contents, 'html.parser')\n",
    "            cleaned_text_2 = re.sub(r'[\\n\\t]', '', soup.text).split(\" \")\n",
    "            cleaned_text_2 = [re.sub(r'[^\\w\\s]', '', word) for word in cleaned_text_2 if word != \"\"]\n",
    "            cleaned_text_2 = [word for word in cleaned_text_2 if word not in eng_stop_words]\n",
    "            cleaned_text_2 = \" \".join([WNL.lemmatize(word.lower()) for word in cleaned_text_2])\n",
    "            t2 = Counter(WORD.findall(cleaned_text_2))\n",
    "\n",
    "            cosine_similarity = get_cosine(t1, t2)\n",
    "\n",
    "            if cosine_similarity > 0.1:\n",
    "                final_url_output.append(record.rec_headers.get_header('WARC-Target-URI'))\n",
    "\n",
    "            if len(final_url_output) == 1000:\n",
    "                break\n",
    "                \n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://1057news.com/2018/09/26/13/13/01/roane-county-experiencing-power-outages-and-flooded-roads/',\n",
       " 'http://107.housedems.com/article/executive-budget-recommendation-summary',\n",
       " 'http://123thebig3.com/2019/10/',\n",
       " 'http://2fwww.openbooktoronto.com/ava_homa/main',\n",
       " 'http://2fwww.openbooktoronto.com/edward_carson/main',\n",
       " 'http://365daysofme.com/2015/05/19/the-chicken-chronicles-32/',\n",
       " 'http://4js.com/2018-french-housing-corp-pursues-digital-transformation/?s=',\n",
       " 'http://academagia.invisionzone.com/topic/3252-a-bittersweet-symphony-the-tale-of-manchester-and-captain-falshaw/?tab=comments',\n",
       " 'http://actionfigurejunkies.com/pet-sematary-final-trailer/',\n",
       " 'http://adamsenger07.mihanblog.com/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_url_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "URL_dict = {'URL_List': final_url_output}\n",
    "df = pd.DataFrame(URL_dict) \n",
    "df.to_csv(\"Final_Output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
